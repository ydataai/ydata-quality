{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# YData Quality - Duplicates Tutorial\n",
    "Time-to-Value: 4 minutes\n",
    "\n",
    "This notebook provides a tutorial for the ydata_quality package funcionality for duplicate values.\n",
    "\n",
    "**Structure:**\n",
    "\n",
    "1. Load dataset\n",
    "2. Distort dataset\n",
    "3. Instantiate the Data Quality engine\n",
    "4. Run the quality checks\n",
    "5. Assess the warnings\n",
    "6. (Extra) Detailed overview"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "from ydata_quality.duplicates import DuplicateChecker"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load the example dataset\n",
    "We will use a transformed version of the \"Guerry\" dataset available from the statsmodels package."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "df = pd.read_csv('../datasets/transformed/guerry_histdata.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create the engine\n",
    "Each engine contains the checks and tests for each suite. To create a DuplicateChecker, you provide:\n",
    "- df: target DataFrame, for which we will run the test suite\n",
    "- entities (optional): list of feature names for which checking duplicates after grouping-by is applicable."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "dc = DuplicateChecker(df=df, entities=['Region', 'MainCity'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Full Evaluation\n",
    "The easiest way to assess the data quality analysis is to run `.evaluate()` which returns a list of warnings for each quality check. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "results = dc.evaluate()\n",
    "results.keys()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[1mWarnings:\u001b[0m\n",
      "\tTOTAL: 3 warning(s)\n",
      "\t\u001b[1m\u001b[38;5;209mPriority 1\u001b[0m: 1 warning(s)\n",
      "\t\u001b[1m\u001b[38;5;11mPriority 2\u001b[0m: 2 warning(s)\n",
      "\n",
      "\u001b[38;5;209m\u001b[1mPriority 1\u001b[0m - \u001b[1mheavy impact expected\u001b[0m:\n",
      "\t\u001b[38;5;209m*\u001b[0m \u001b[1m[DUPLICATES\u001b[0m - \u001b[4mDUPLICATE COLUMNS]\u001b[0m Found 1 columns with exactly the same feature values as other columns.\n",
      "\u001b[38;5;11m\u001b[1mPriority 2\u001b[0m - \u001b[1musage allowed, limited human intelligibility\u001b[0m:\n",
      "\t\u001b[38;5;11m*\u001b[0m \u001b[1m[DUPLICATES\u001b[0m - \u001b[4mENTITY DUPLICATES]\u001b[0m Found 20 duplicates after grouping by entities.\n",
      "\t\u001b[38;5;11m*\u001b[0m \u001b[1m[DUPLICATES\u001b[0m - \u001b[4mEXACT DUPLICATES]\u001b[0m Found 20 instances with exact duplicate feature values.\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_keys(['exact_duplicates', 'entity_duplicates', 'duplicate_columns'])"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Check the status\n",
    "After running the data quality checks, you can check the warnings for each individual test. The warnings are suited by priority and have additional details that can provide better insights for Data Scientists."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Retrieve the warnings\n",
    "warnings = dc.get_warnings()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Full Test Suite\n",
    "In this section, you will find a detailed overview of the available tests in the duplicates module of ydata_quality."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exact Duplicates\n",
    "\n",
    "We consider exact duplicates the rows which contain the exact same feature values for more than 1 row.\n",
    "\n",
    "The return is a DataFrame containing the duplicate instances, not containing the original (i.e. first seen) rows."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "exact_duplicates_out = dc.exact_duplicates()\n",
    "exact_duplicates_out.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Entity Duplicates\n",
    "We define an _entity_ as any feature value for which a groupby-aggregation would make sense (e.g. categoricals).\n",
    "\n",
    "Entity duplicates exist when we have exactly the same rows after grouping by a given entity. Entity duplicates are by definition exact duplicates, but this perspective allows to isolate the grouping of interest (i.e. the groupby for which we have duplicates).\n",
    "\n",
    "You can either specify the given entities for checking duplicates or default to the entities set in DuplicateChecker init."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "given_entity_duplicates_out = dc.entity_duplicates('MainCity')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dc.entities = ['Region']\n",
    "entity_duplicates_out = dc.entity_duplicates()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# If the entities are not specified, the test will be skipped.\n",
    "dc.entities = []\n",
    "dc.entity_duplicates()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# When passed a composed entity, get the duplicates grouped by value intersection\n",
    "dc.entities = [['Region', 'MainCity']]\n",
    "composed_entity_duplicates_out = dc.entity_duplicates()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Column Duplicates\n",
    "We define a column duplicate as any column that contains the exactly same feature values as another column in the same DataFrame."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dc.duplicate_columns()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cdc2bce73c2a9ac283f602628cabf735dbe06c4ee87a7849fc5f3d1177c8f304"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('.venv': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "cdc2bce73c2a9ac283f602628cabf735dbe06c4ee87a7849fc5f3d1177c8f304"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}