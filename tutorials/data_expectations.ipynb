{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# YData Quality - Data Expectations Tutorial\n",
    "Time-to-Value: 8 minutes\n",
    "\n",
    "This notebook provides a tutorial for the ydata_quality package integration of the Great Expectations library for managing data expectations.\n",
    "\n",
    "**Structure:**\n",
    "\n",
    "1. A data expectations introduction\n",
    "2. Load example dataset\n",
    "3. Instantiate the Data Quality engine\n",
    "4. Run the quality checks\n",
    "5. Assess the warnings\n",
    "6. (Extra) Detailed overview"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## A data expectations introduction\n",
    "### What are data expectations?\n",
    "Detecting inconsistencies or even errors in data can sometimes be a trivial task, but surely this is far from being the norm. Many times this task requires minucious inspection of lots of data structures or advanced domain knowledge that allows a user to confidently label any shortcoming.\n",
    "\n",
    "Consider __[Test-Driven-Development](https://en.wikipedia.org/wiki/Test-driven_development)__ (TDD) for a moment. In a TDD process, software requirements are realized into test cases before the development of the software itself. Software changes are constantly ran against these test cases in order to, hopefully, detect any sort of problem that might occur. A full software pipeline can be tested in this fashion to establish a quality assurance protocol, warrant a green light for a production push, and supporting refactorizations.\n",
    "\n",
    "But what about data? What if you could generalize domain knowledge, and generally expected data behaviour, into the datasets you manipulate, either internally sourced or from third parties? In fact many teams already do this in one way or another, but generally resorting to ad-hoc and hard to generalize processes. Taking the lesson from TDD, if we could develop a set of verifiable tests that work just like software test cases we would also get the same benefits.\n",
    "\n",
    "**Data Expectation** is the name we use for unit tests applied to data, to define an expectation about data is to develop a unit test that asserts a certain property about the data and provides an actionable output in any deviation.\n",
    "\n",
    "### What is Great Expectations?\n",
    "__[Great Expectations](https://greatexpectations.io/)__ is a Python tool for creating and running data expectations suite, allowing you to validate, profile your data, automate report creation in the form of HTML documents and store validation logs. Great Expectations offers a wide range of built-in expectations but also allows you to define custom expectations that better fit to your needs.\n",
    "\n",
    "### How can I leverage my Great Expectations project with YData Quality?\n",
    "Its simple!\n",
    "\n",
    "1. Locate the validations directory of your Great Expectations project, which should be under the *uncommitted* directory. There you will find a set of folders, one for each validation run that you executed.\n",
    "2. Choose a validation run to which you would like to get more insight, and copy the path to the json file.\n",
    "3. Instantiate a DataExpectationsReporter engine and run evaluate by providing the json file path.\n",
    "\n",
    "Congratulations you are all set!\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "\n",
    "from ydata_quality.data_expectations import DataExpectationsReporter"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load the example dataset and path to Great Expectations validation run\n",
    "We will use a demo from the GE tutorials. The taxi ride dataset and a log from a validation run on this dataset."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# This is the DataFrame used in the demo from GE tutorials\n",
    "df = pd.read_csv('../src/ydata_quality/data_expectations/test_cases/taxi/yellow_tripdata_sample_2019-01.csv')\n",
    "\n",
    "# This is a sample json log taken from a validation run\n",
    "results_json_path = '../src/ydata_quality/data_expectations/test_cases/taxi/long.json'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create the engine\n",
    "Each engine contains the checks and tests for each suite."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "der = DataExpectationsReporter()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Full Evaluation\n",
    "The easiest way to assess the data quality analysis is to run `.evaluate()` which returns a dictionary with outputs of operation performed. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "results = der.evaluate(results_json_path, df)\n",
    "\n",
    "\n",
    "results.keys()  "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/fsantos/miniconda3/envs/DQ/lib/python3.8/site-packages/pandas/core/internals/blocks.py:1002: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr_value = np.array(value)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_keys(['Overall Assessment', 'Coverage Fraction', 'Expectation level assessment'])"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Check the status\n",
    "After running the data quality checks, you can check the warnings for each individual operation over the GE validation log. The warnings are sorted by priority and have additional details that can provide better insights for Data Scientists."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "der.report()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[EXPECTATION ASSESSMENT - VALUE BETWEEN] The observed value is outside of the expected range.\n",
      "\t- The observed value is -100% deviated from the nearest bound of the expected range. (Priority 3: minor impact, aesthetic)\n",
      "[EXPECTATION ASSESSMENT - VALUE BETWEEN] The observed value is outside of the expected range.\n",
      "\t- The observed value is -14% deviated from the nearest bound of the expected range. (Priority 3: minor impact, aesthetic)\n",
      "[EXPECTATION ASSESSMENT - VALUE BETWEEN] The observed value is outside of the expected range.\n",
      "\t- The observed value is 17% deviated from the nearest bound of the expected range. (Priority 3: minor impact, aesthetic)\n",
      "[OVERALL ASSESSMENT] 10 expectations have failed, which is more than the implied absolute threshold of 0 failed expectations. (Priority 2: usage allowed, limited human intelligibility)\n",
      "[EXPECTATION ASSESSMENT - VALUE BETWEEN] The observed value is outside of the expected range.\n",
      "\t- The observed value is -35% deviated from the nearest bound of the expected range. (Priority 3: minor impact, aesthetic)\n",
      "[EXPECTATION ASSESSMENT - VALUE BETWEEN] The observed value is outside of the expected range.\n",
      "\t- The observed value is 5% deviated from the nearest bound of the expected range. (Priority 3: minor impact, aesthetic)\n",
      "[EXPECTATION ASSESSMENT - VALUE BETWEEN] The observed value is outside of the expected range.\n",
      "\t- The observed value is 3% deviated from the nearest bound of the expected range. (Priority 3: minor impact, aesthetic)\n",
      "[COVERAGE FRACTION] The provided DataFrame has a total expectation coverage of 11% of its columns, which is below the expected coverage of 75%. (Priority 2: usage allowed, limited human intelligibility)\n",
      "[EXPECTATION ASSESSMENT - VALUE BETWEEN] The observed value is outside of the expected range.\n",
      "\t- The observed value is 1% deviated from the nearest bound of the expected range. (Priority 3: minor impact, aesthetic)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Quality Warning"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Get a sample warning\n",
    "sample_warning = list(der.warnings)[1]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# Check the details\n",
    "sample_warning.test, sample_warning.description, sample_warning.priority"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('Expectation assessment - Value Between',\n",
       " 'The observed value is outside of the expected range.\\n\\t- The observed value is -14% deviated from the nearest bound of the expected range.',\n",
       " <Priority.P3: 3>)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# Retrieve the relevant data from the warning\n",
    "sample_warning_data = sample_warning.data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Full Test Suite\n",
    "In this section, you will find a detailed overview of the available tests in the data expectations module of ydata_quality."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e255f3ac955330aecee05fff6b7b15b68f4bd4cf0e9481cf0822c8a2e5228d43"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('DQ': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "metadata": {
   "interpreter": {
    "hash": "cdc2bce73c2a9ac283f602628cabf735dbe06c4ee87a7849fc5f3d1177c8f304"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}