{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# YData Quality - Data Expectations Tutorial\n",
    "Time-to-Value: 8 minutes\n",
    "\n",
    "This notebook provides a tutorial for the ydata_quality package integration of the Great Expectations library for managing data expectations.\n",
    "\n",
    "**Structure:**\n",
    "\n",
    "1. Load Great Expectations validation run\n",
    "2. Instantiate the Data Quality engine\n",
    "3. Run the quality checks\n",
    "4. Assess the warnings\n",
    "5. (Extra) Detailed overview"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## A data expectations introduction\n",
    "### What are data expectations?\n",
    "Detecting inconsistencies or even errors in data can sometimes be a trivial task, but surely this is far from being the norm. Many times this task requires minucious inspection of lots of data structures or advanced domain knowledge that allows a user to confidently label any shortcoming.\n",
    "\n",
    "Consider __[Test-Driven-Development](https://en.wikipedia.org/wiki/Test-driven_development)__ (TDD) for a moment. In a TDD process, software requirements are realized into test cases before the development of the software itself. Software changes are constantly ran against these test cases in order to, hopefully, detect any sort of problem that might occur. A full software pipeline can be tested in this fashion to warrant a green light for a production push, establishing a quality assurance protocol with a end user or supporting refactorizations.\n",
    "\n",
    "But what about data? What if you could generalize domain knowledge, and generally expected data behaviour, into the datasets you manipulate, either internally sourced or from third parties? In fact many teams already do this in one way or another, but taking the lesson from TDD, if we could easily develop a set of verifiable tests that work just like software test cases we would also get the same benefits. **Data Expectation** is the name we use for unit tests applied to data, to define an expectation about data is to develop a unit test that asserts a certain property about the data and provides an actionable output in any deviation.\n",
    "\n",
    "### What is Great Expectations?\n",
    "__[Great Expectations](https://greatexpectations.io/)__ is a Python tool for creating and running data expectations suite, allowing you to validate, profile your data and automate report creation in the form of HTML documents. Great Expectations offers a wide range of built-in expectations but also allows you to define custom expectations that better fit to your needs.\n",
    "\n",
    "### How can I leverage my Great Expectations project with YData Quality?\n",
    "Its simple!\n",
    "\n",
    "Locate the validations directory of your Great Expectations project, which should be under uncommitted. There you will find a set of folders, one for each validation run that you executed. Choose a validation run to which you would like to get more insight, and provide its path to the DataExpectationsReporter or as a loaded json (using the native json package loads method per example). Instantiate the engine and run evaluate. Congratulations you are all set!\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import statsmodels.api as sm\n",
    "from ydata_quality.data_errors import DataErrorSearcher"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load the example dataset\n",
    "We will use a dataset available from the statsmodels package."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = sm.datasets.get_rdataset('Guerry', 'HistData').data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Distort the original dataset\n",
    "Apply transformations to highlight the data quality functionalities."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Duplicate the first 20 rows\n",
    "df = df.append(df[:20], ignore_index=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Duplicate the dept column\n",
    "df[\"dept2\"] = df[\"dept\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create the engine\n",
    "Each engine contains the checks and tests for each suite. To create a DataErrorSearcher, you provide:\n",
    "- df: target DataFrame, for which we will run the test suite"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "des = DataErrorSearcher(df=df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Full Evaluation\n",
    "The easiest way to assess the data quality analysis is to run `.evaluate()` which returns a list of warnings for each quality check. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "results = des.evaluate()\n",
    "results.keys()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Check the status\n",
    "After running the data quality checks, you can check the warnings for each individual test. The warnings are suited by priority and have additional details that can provide better insights for Data Scientists."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "des.report()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Quality Warning"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Get a sample warning\n",
    "sample_warning = des.warnings[1]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Check the details\n",
    "sample_warning.test, sample_warning.description, sample_warning.priority"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Retrieve the relevant data from the warning\n",
    "sample_warning_data = sample_warning.data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Full Test Suite\n",
    "In this section, you will find a detailed overview of the available tests in the data errors module of ydata_quality."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e255f3ac955330aecee05fff6b7b15b68f4bd4cf0e9481cf0822c8a2e5228d43"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('DQ': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "metadata": {
   "interpreter": {
    "hash": "cdc2bce73c2a9ac283f602628cabf735dbe06c4ee87a7849fc5f3d1177c8f304"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}