{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# YData Quality - Bias & Fairness Tutorial\n",
    "Time-to-Value: 3 minutes\n",
    "\n",
    "This notebook provides a tutorial for the ydata_quality package module on Bias & Fairness .\n",
    "\n",
    "**Structure:**\n",
    "\n",
    "1. A bias and fairness introduction\n",
    "2. Load example dataset\n",
    "3. Instantiate the Data Quality engine\n",
    "4. Run the quality checks\n",
    "5. Assess the warnings\n",
    "6. (Extra) Detailed overview"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bias and Fairness\n",
    "As data is increasingly used for automated decision making with heavy impact on individual lifes, a thorough analysis of data quality encompasses an understanding of the inherent biases embedded in the datasets that can cause different treatments based on sensitive attributes. There are ethical and legal obligations for Data Scientists to develop applications which are unbiased and fair.\n",
    "\n",
    "### Definitions\n",
    "We consider _fairness_ to be the absence of differentiated treatment (assistive or punitive) based on sensitive attributes. _Fairness_ can also be thought of as the absence of unjustified basis for differentiated treatment.\n",
    "\n",
    "We consider _sensitive attributes (a.k.a. sensitive features)_ as personal details for which there are legal and ethical obligations for which not to differentiate the treatment based on.\n",
    "\n",
    "We consider _bias_ as a systematic, non-neglectable treatment which is differentiated towards a specific sub-group of individuals.\n",
    "\n",
    "### Remarks\n",
    "- The absence of sensitive attributes in a data application does not guarantee fairness by default. \n",
    "- Biases in data can be originated from multiple sources: sample (e.g. fair representation of multiple groups), label (e.g. how the outcome was defined), machine learning pipeline (e.g. how data is diggested), application (e.g. how an application is deployed). Not all biases are available during data quality analysis but it is important to keep them in mind from the beginning.\n",
    "\n",
    "\n",
    "\n",
    "**References**\n",
    "- Chapter 11 Bias and Fairness | Big Data and Social Science [(link)](https://textbook.coleridgeinitiative.org/chap-bias.html)\n",
    "- Fairness and Machine Learning - Limitations and Opportunities [(link)](https://fairmlbook.org/)\n",
    "- Fairness Tutorial - Moritz Hardt - MLSS 2020, TÃ¼bingen [(link)](https://www.youtube.com/watch?v=Igq_S_7IfOU)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "from ydata_quality.bias_fairness import BiasFairness"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load the example dataset\n",
    "The \"Adult Data Set\" (a.k.a. \"Census Income\") contains a set of records to predict whether an individual's income exceeds $50K/yr, based on census data. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# This is the DataFrame used in the demo from GE tutorials\n",
    "df = pd.read_csv('../datasets/transformed/census_10k.csv')\n",
    "df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   age workclass  fnlwgt  education  education-num      marital-status  \\\n",
       "0   57   Private  132704    Masters             14           Separated   \n",
       "1   25   Private  321205  Bachelors             13       Never-married   \n",
       "2   25   Private   38090  Bachelors             13       Never-married   \n",
       "3   54   Private   29909       11th              7  Married-civ-spouse   \n",
       "4   43   Private  347934    HS-grad              9           Separated   \n",
       "\n",
       "        occupation   relationship   race     sex  capital-gain  capital-loss  \\\n",
       "0   Prof-specialty  Not-in-family  White    Male         10520             0   \n",
       "1  Exec-managerial  Not-in-family  White    Male          4101             0   \n",
       "2            Sales  Not-in-family  White    Male             0             0   \n",
       "3    Other-service           Wife  White  Female             0             0   \n",
       "4            Sales  Not-in-family  White  Female             0             0   \n",
       "\n",
       "   hours-per-week native-country income workclass2  \n",
       "0              32  United-States   >50K    Private  \n",
       "1              35  United-States  <=50K    Private  \n",
       "2              44  United-States  <=50K    Private  \n",
       "3              43  United-States  <=50K    Private  \n",
       "4              30  United-States  <=50K    Private  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "      <th>workclass2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57</td>\n",
       "      <td>Private</td>\n",
       "      <td>132704</td>\n",
       "      <td>Masters</td>\n",
       "      <td>14</td>\n",
       "      <td>Separated</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>10520</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "      <td>Private</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25</td>\n",
       "      <td>Private</td>\n",
       "      <td>321205</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>4101</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>Private</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25</td>\n",
       "      <td>Private</td>\n",
       "      <td>38090</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>Private</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54</td>\n",
       "      <td>Private</td>\n",
       "      <td>29909</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>Private</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43</td>\n",
       "      <td>Private</td>\n",
       "      <td>347934</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Separated</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>Private</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create the engine\n",
    "Each engine contains the checks and tests for each suite."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "bf = BiasFairness(df=df, sensitive_features=['race', 'sex'], label='income', random_state=42)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Full Evaluation\n",
    "The easiest way to assess the data quality analysis is to run `.evaluate()` which returns a dictionary with outputs of operation performed."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "results = bf.evaluate()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[1mWarnings:\u001b[0m\n",
      "\tTOTAL: 2 warning(s)\n",
      "\t\u001b[1m\u001b[38;5;11mPriority 2\u001b[0m: 2 warning(s)\n",
      "\n",
      "\n",
      "\u001b[38;5;11m\u001b[1mPriority 2\u001b[0m - \u001b[1musage allowed, limited human intelligibility\u001b[0m:\n",
      "\t\u001b[38;5;11m*\u001b[0m \u001b[1m[BIAS&FAIRNESS\u001b[0m - \u001b[4mPROXY IDENTIFICATION]\u001b[0m Found 1 feature pairs of correlation to sensitive attributes with values higher than defined threshold (0.5).\n",
      "\t\u001b[38;5;11m*\u001b[0m \u001b[1m[BIAS&FAIRNESS\u001b[0m - \u001b[4mSENSITIVE ATTRIBUTE REPRESENTATIVITY]\u001b[0m Found 2 values of 'race' sensitive attribute with low representativity in the dataset (below 1.00%).\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "After running the data quality checks, you can check the warnings for each individual test of the Bias & Fairness module. The warnings are sorted by priority and have additional details that can provide better insights for Data Scientists."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "bias_fairness_warnings = bf.get_warnings()\n",
    "bias_fairness_warnings"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[QualityWarning(category='Bias&Fairness', test='Proxy Identification', description='Found 1 feature pairs of correlation to sensitive attributes with values higher than defined threshold (0.5).', priority=<Priority.P2: 2>, data=features\n",
       " relationship_sex    0.650656\n",
       " Name: association, dtype: float64),\n",
       " QualityWarning(category='Bias&Fairness', test='Sensitive Attribute Representativity', description=\"Found 2 values of 'race' sensitive attribute with low representativity in the dataset (below 1.00%).\", priority=<Priority.P2: 2>, data=Other                 0.0092\n",
       " Amer-Indian-Eskimo    0.0090\n",
       " Name: race, dtype: float64)]"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Full Test Suite\n",
    "In this section, you will find a detailed overview of the available tests in the data expectations module of ydata_quality. These are all run with the `evaluate` method, which centralizes input arguments and produces specific outputs in the returned results dictionary, structured by test."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Results object structure\n",
    "list(results.keys())"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['performance_discrimination',\n",
       " 'proxy_identification',\n",
       " 'sensitive_predictability',\n",
       " 'sensitive_representativity']"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Performance Discrimination\n",
    "The \"Performance Discrimination\" inspects the disparities in the classifying performance across values of sensitive attributes. A model is trained on the full data but the performance metrics are broken down per each sub-group, enabling the Data Scientist to understand the performance of a baseline classifier throughout each partition of sub-group data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "performances = bf.performance_discrimination()\n",
    "performances"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'race': Amer-Indian-Eskimo    1.000000\n",
       " Asian-Pac-Islander    0.510216\n",
       " Other                 1.000000\n",
       " Black                 0.639949\n",
       " White                 0.562961\n",
       " dtype: float64,\n",
       " 'sex': Female    0.526722\n",
       " Male      0.591663\n",
       " dtype: float64}"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Proxy Identification\n",
    "The \"Proxy Identification\" test aims to detect associations between sensitive and non-sensitive attributes in the data, to signal Data Scientists for unwanted proxies. The leakage of protected features by non-protected ones may give Data Scientists a false confidence that by disregarding sensitive attributes that they will not be available in the data (directly or indirectly)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "bf.proxy_identification(th=0.2)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "features\n",
       "relationship_sex       0.650656\n",
       "marital-status_sex     0.459833\n",
       "occupation_sex         0.426367\n",
       "native-country_race    0.402390\n",
       "hours-per-week_sex     0.235445\n",
       "income_sex             0.208361\n",
       "Name: association, dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sensitive Attributes Predictability\n",
    "The \"Sensitive Attributess Predictability\"  test builds a predictive model with sensitive attributes as targets to grasp how easy it is to build a baseline that can predict protected features based on non-protected ones. Similar to the \"Proxy Identification\" mechanism with more complex relationships (based on a model, not on association measures). The leakage of protected features by non-protected ones may give Data Scientists a false confidence that by disregarding sensitive attributes that they will not be available in the data (directly or indirectly).\n",
    "\n",
    "The performance metric values are calculated as a ratio of _(real - min)/(max - min)_. The best performance (max) is defined as a perfect prediction, where the predictions equal the true outcomes. The baseline performance (min) is defined as the performance achieved by a naive model, where the predictions are the mode for classification or mean for regression.\n",
    "\n",
    "The values returned by the test indicate the percentage of achievable performance achieved by a baseline model trained to predict a sensitive attribute."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "sens_pred = bf.sensitive_predictability()\n",
    "sens_pred"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "race     0.12168\n",
       "sex     0.249346\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "\n",
    "bf.sensitive_predictability(adjusted_metric=False)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "race    0.049889\n",
       "sex     0.624673\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sensitive Attributes Representativity\n",
    "The \"Sensitive Attributes Representativity\" calculates the distribution of categorical features to assess if any sub-group of a sensitive attribute is underrepresented. It is meant for Data Scientists to validate the data for sampling bias, i.e. the systematic over/under representation of some members of a population in relation to the others. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "bf.sensitive_representativity()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'race': White                 0.8537\n",
       " Black                 0.0978\n",
       " Asian-Pac-Islander    0.0303\n",
       " Other                 0.0092\n",
       " Amer-Indian-Eskimo    0.0090\n",
       " Name: race, dtype: float64,\n",
       " 'sex': Male      0.6657\n",
       " Female    0.3343\n",
       " Name: sex, dtype: float64}"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cdc2bce73c2a9ac283f602628cabf735dbe06c4ee87a7849fc5f3d1177c8f304"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('.venv': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "cdc2bce73c2a9ac283f602628cabf735dbe06c4ee87a7849fc5f3d1177c8f304"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}