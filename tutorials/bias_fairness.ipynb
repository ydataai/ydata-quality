{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# YData Quality - Bias & Fairness Tutorial\n",
    "Time-to-Value: 3 minutes\n",
    "\n",
    "This notebook provides a tutorial for the ydata_quality package module on Bias & Fairness .\n",
    "\n",
    "**Structure:**\n",
    "\n",
    "1. A bias and fairness introduction\n",
    "2. Load example dataset\n",
    "3. Instantiate the Data Quality engine\n",
    "4. Run the quality checks\n",
    "5. Assess the warnings\n",
    "6. (Extra) Detailed overview"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bias and Fairness\n",
    "As data is increasingly used for automated decision making with heavy impact on individual lifes, a thorough analysis of data quality encompasses an understanding of the inherent biases embedded in the datasets that can cause different treatments based on sensitive attributes. There are ethical and legal obligations for Data Scientists to develop applications which are unbiased and fair.\n",
    "\n",
    "### Definitions\n",
    "We consider _fairness_ to be the absence of differentiated treatment (assistive or punitive) based on sensitive attributes. _Fairness_ can also be thought of as the absence of unjustified basis for differentiated treatment.\n",
    "\n",
    "We consider _sensitive attributes (a.k.a. sensitive features)_ as personal details for which there are legal and ethical obligations for which not to differentiate the treatment based on.\n",
    "\n",
    "We consider _bias_ as a systematic, non-neglectable treatment which is differentiated towards a specific sub-group of individuals.\n",
    "\n",
    "### Remarks\n",
    "- The absence of sensitive attributes in a data application does not guarantee fairness by default. \n",
    "- Biases in data can be originated from multiple sources: sample (e.g. fair representation of multiple groups), label (e.g. how the outcome was defined), machine learning pipeline (e.g. how data is diggested), application (e.g. how an application is deployed). Not all biases are available during data quality analysis but it is important to keep them in mind from the beginning.\n",
    "\n",
    "\n",
    "\n",
    "**References**\n",
    "- Chapter 11 Bias and Fairness | Big Data and Social Science [(link)](https://textbook.coleridgeinitiative.org/chap-bias.html)\n",
    "- Fairness and Machine Learning - Limitations and Opportunities [(link)](https://fairmlbook.org/)\n",
    "- Fairness Tutorial - Moritz Hardt - MLSS 2020, TÃ¼bingen [(link)](https://www.youtube.com/watch?v=Igq_S_7IfOU)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "\n",
    "from ydata_quality.bias_fairness import BiasFairness"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load the example dataset\n",
    "The \"Adult Data Set\" (a.k.a. \"Census Income\") contains a set of records to predict whether an individual's income exceeds $50K/yr, based on census data. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# This is the DataFrame used in the demo from GE tutorials\n",
    "df = pd.read_csv('../datasets/transformed/census_10k.csv')\n",
    "df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          workclass  fnlwgt   education  education-num  \\\n",
       "0   39          State-gov   77516   Bachelors             13   \n",
       "1   50   Self-emp-not-inc   83311   Bachelors             13   \n",
       "2   38            Private  215646     HS-grad              9   \n",
       "3   53            Private  234721        11th              7   \n",
       "4   28            Private  338409   Bachelors             13   \n",
       "\n",
       "        marital-status          occupation    relationship    race      sex  \\\n",
       "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
       "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
       "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
       "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
       "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
       "\n",
       "   capital-gain  capital-loss  hours-per-week  native-country  income  \n",
       "0          2174             0              40   United-States   <=50K  \n",
       "1             0             0              13   United-States   <=50K  \n",
       "2             0             0              40   United-States   <=50K  \n",
       "3             0             0              40   United-States   <=50K  \n",
       "4             0             0              40            Cuba   <=50K  "
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create the engine\n",
    "Each engine contains the checks and tests for each suite."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "bf = BiasFairness(df=df, sensitive_features=['race', 'sex'], label='income', random_state=42)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Full Evaluation\n",
    "The easiest way to assess the data quality analysis is to run `.evaluate()` which returns a dictionary with outputs of operation performed. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "results = bf.evaluate()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Check the status\n",
    "After running the data quality checks, you can check the warnings for each individual test of the Bias & Fairness module. The warnings are sorted by priority and have additional details that can provide better insights for Data Scientists."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "bf.report()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Warnings count by priority:\n",
      "\tPriority 2: 2 warning(s)\n",
      "\tTOTAL: 2 warning(s)\n",
      "List of warnings sorted by priority:\n",
      "\t[PROXY IDENTIFICATION] Found 1 feature pairs of correlation to sensitive attributes with values higher than defined threshold (0.5). (Priority 2: usage allowed, limited human intelligibility)\n",
      "\t[SENSITIVE ATTRIBUTE REPRESENTATIVITY] Found 2 values of 'race' sensitive attribute with low representativity in the dataset (below 1.00%). (Priority 2: usage allowed, limited human intelligibility)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "\n",
    "bias_fairness_warnings = bf.get_warnings()\n",
    "bias_fairness_warnings"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[QualityWarning(category='Bias&Fairness', test='Proxy Identification', description='Found 1 feature pairs of correlation to sensitive attributes with values higher than defined threshold (0.5).', priority=<Priority.P2: 2>, data=features\n",
       " relationship_sex    0.643254\n",
       " Name: association, dtype: float64),\n",
       " QualityWarning(category='Bias&Fairness', test='Sensitive Attribute Representativity', description=\"Found 2 values of 'race' sensitive attribute with low representativity in the dataset (below 1.00%).\", priority=<Priority.P2: 2>, data= Amer-Indian-Eskimo    0.0099\n",
       "  Other                 0.0083\n",
       " Name: race, dtype: float64)]"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Full Test Suite\n",
    "In this section, you will find a detailed overview of the available tests in the data expectations module of ydata_quality. These are all run with the `evaluate` method, which centralizes input arguments and produces specific outputs in the returned results dictionary, structured by test."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Results object structure\n",
    "list(results.keys())"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['performance_discrimination',\n",
       " 'proxy_identification',\n",
       " 'sensitive_predictability',\n",
       " 'sensitive_representativity']"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Performance Discrimination\n",
    "The \"Performance Discrimination\" inspects the disparities in the classifying performance across values of sensitive attributes. A model is trained on the full data but the performance metrics are broken down per each sub-group, enabling the Data Scientist to understand the performance of a baseline classifier throughout each partition of sub-group data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "performances = bf.performance_discrimination()\n",
    "performances"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'race':  Other                 [ERROR] Failed performance metric with message...\n",
       "  Amer-Indian-Eskimo                                             0.572917\n",
       "  Asian-Pac-Islander                                             0.550847\n",
       "  White                                                          0.573228\n",
       "  Black                                                          0.595126\n",
       " dtype: object,\n",
       " 'sex':  Male      0.582815\n",
       "  Female    0.614891\n",
       " dtype: float64}"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Proxy Identification\n",
    "The \"Proxy Identification\" test aims to detect associations between sensitive and non-sensitive attributes in the data, to signal Data Scientists for unwanted proxies. The leakage of protected features by non-protected ones may give Data Scientists a false confidence that by disregarding sensitive attributes that they will not be available in the data (directly or indirectly)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "bf.proxy_identification(th=0.2)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "features\n",
       "relationship_sex       0.643254\n",
       "marital-status_sex     0.449933\n",
       "occupation_sex         0.420299\n",
       "native-country_race    0.409056\n",
       "hours-per-week_sex     0.224377\n",
       "income_sex             0.202520\n",
       "Name: association, dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sensitive Attributes Predictability\n",
    "The \"Sensitive Attributess Predictability\"  test builds a predictive model with sensitive attributes as targets to grasp how easy it is to build a baseline that can predict protected features based on non-protected ones. Similar to the \"Proxy Identification\" mechanism with more complex relationships (based on a model, not on association measures). The leakage of protected features by non-protected ones may give Data Scientists a false confidence that by disregarding sensitive attributes that they will not be available in the data (directly or indirectly).\n",
    "\n",
    "The performance metric values are calculated as a ratio of _(real - min)/(max - min)_. The best performance (max) is defined as a perfect prediction, where the predictions equal the true outcomes. The baseline performance (min) is defined as the performance achieved by a naive model, where the predictions are the mode for classification or mean for regression.\n",
    "\n",
    "The values returned by the test indicate the percentage of achievable performance achieved by a baseline model trained to predict a sensitive attribute."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "sens_pred = bf.sensitive_predictability()\n",
    "sens_pred"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "race    0.120460\n",
       "sex     0.254559\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "\n",
    "bf.sensitive_predictability(adjusted_metric=False)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "race    0.050544\n",
       "sex     0.627280\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sensitive Attributes Representativity\n",
    "The \"Sensitive Attributes Representativity\" calculates the distribution of categorical features to assess if any sub-group of a sensitive attribute is underrepresented. It is meant for Data Scientists to validate the data for sampling bias, i.e. the systematic over/under representation of some members of a population in relation to the others. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "bf.sensitive_representativity()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'race':  White                 0.8556\n",
       "  Black                 0.0953\n",
       "  Asian-Pac-Islander    0.0309\n",
       "  Amer-Indian-Eskimo    0.0099\n",
       "  Other                 0.0083\n",
       " Name: race, dtype: float64,\n",
       " 'sex':  Male      0.6703\n",
       "  Female    0.3297\n",
       " Name: sex, dtype: float64}"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e255f3ac955330aecee05fff6b7b15b68f4bd4cf0e9481cf0822c8a2e5228d43"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('DQ': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "metadata": {
   "interpreter": {
    "hash": "cdc2bce73c2a9ac283f602628cabf735dbe06c4ee87a7849fc5f3d1177c8f304"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
