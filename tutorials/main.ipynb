{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# ydata-quality\n",
    "> The *ydata_quality* package aims to be for Data Quality what *sklearn* is for machine learning, *matplotlib* for visualization or *pandas* for data manipulation.\n",
    "\n",
    "The `ydata_quality` package aims to be the the go-to package for assessing Data Quality throughout the multiple stages of a data pipeline development. Once you have a dataset available, running `DataQuality(df=my_df).report()` provides a comprehensive overview of the details and intricacies of the data, through the perspective of the multiple modules available in the package.\n",
    "\n",
    "For this tutorial, we will 1. load a dataset; 2. analyze its quality issues; 3. apply strategies to mitigate them and 4. check the new quality analysis on the post-processed (cleaned) data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Quick Start\n",
    "Load a DataFrame and evaluate your data using `DataQuality`.\n",
    "For more advanced analysis, we can provide additional arguments but we will get there in a minute."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "%%capture\n",
    "from ydata_quality import DataQuality\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(f'../datasets/transformed/census_10k_v3.csv') # load data\n",
    "dq = DataQuality(df=df) # create the main class that holds all quality modules\n",
    "results = dq.evaluate() # run the tests"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "dq.report() # Output a report of the quality issues found by the engines"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Warnings count by priority:\n",
      "\tPriority 1: 1 warning(s)\n",
      "\tPriority 2: 3 warning(s)\n",
      "\tTOTAL: 4 warning(s)\n",
      "List of warnings sorted by priority:\n",
      "\t[DUPLICATE COLUMNS] Found 1 columns with exactly the same feature values as other columns. (Priority 1: heavy impact expected)\n",
      "\t[PREDEFINED VALUED MISSING VALUES] Found 1960 vmvs in the dataset. (Priority 2: usage allowed, limited human intelligibility)\n",
      "\t[FLATLINES] Found 4627 flatline events with a minimun length of 5 among the columns {'relationship', 'workclass2', 'workclass', 'marital-status', 'capital-gain', 'capital-loss', 'sex', 'education-num', 'income', 'education', 'native-country', 'occupation', 'hours-per-week', 'race'}. (Priority 2: usage allowed, limited human intelligibility)\n",
      "\t[EXACT DUPLICATES] Found 3 instances with exact duplicate feature values. (Priority 2: usage allowed, limited human intelligibility)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the report, we get multiple warnings with different priorities (lower is of higher importance). The warnings were generated automatically by the default tests implemented in each module. From the report, we get an overall sense for each quality warning but this doesn't tell the whole story. To investigate the details, let's pick a QualityWarning and analyze it."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Warnings\n",
    "The warnings contain details for issues detected during the data quality analysis. For any given issue, the warning contains the information required by the Data Scientist to fully grasp the quality issue and how it is impacting the current dataset.\n",
    "\n",
    "Warnings can be fetched with `.get_warnings()` as a list of `QualityWarning`'s (best for coding) or summarized with `.report()` which print of overall status (best for analysis, visualization).\n",
    "\n",
    "Warnings are generated automatically for some tests but can be created by Data Scientists as well and added to existing engines."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "dq.get_warnings(test='Duplicate Columns').data"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'workclass': 'workclass2'}"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the warning details, we know that the 'workclass2' feature is an exact copy of the original 'workclass'. For a typical machine learning pipeline, the duplicated feature 'workclass2' could be dropped as it is not adding any value (all info is already present) and may cause a toll in performance (e.g. due to collinearity effects)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Modules\n",
    "A full picture of data quality requires multiple perspectives, which we deliver in a modular way: Bias & Fairness, Data Expectations, Data Relations, Drift Analysis, Labelling, Missing and Valued Missing Values. All of the engines are integrated into a single `DataQuality` class that allows you to run everything at once, providing a holistic perspective of your data. From the main `DataQuality` you can access the individual engines with the `.engines` property.\n",
    "\n",
    "Some of the modules will not run unless you specify specific arguments due to mandatory info (e.g. target feature name for Labelling). By default, `DataQuality` will only contain the engines which have valid arguments and will drop all of those who are not sufficiently specified on the initialization.\n",
    "\n",
    "Since we didn't specified any sensitive features, the Bias&Fairness engine didn't run but we can define it now as a standalone as well."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "from ydata_quality.bias_fairness import BiasFairness\n",
    "bf = BiasFairness(df=df, sensitive_features=['race', 'sex'], label='income')\n",
    "bf_results = bf.evaluate()\n",
    "bf.report()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Warnings count by priority:\n",
      "\tPriority 2: 2 warning(s)\n",
      "\tTOTAL: 2 warning(s)\n",
      "List of warnings sorted by priority:\n",
      "\t[PROXY IDENTIFICATION] Found 1 feature pairs of correlation to sensitive attributes with values higher than defined threshold (0.5). (Priority 2: usage allowed, limited human intelligibility)\n",
      "\t[SENSITIVE ATTRIBUTE REPRESENTATIVITY] Found 2 values of 'race' sensitive attribute with low representativity in the dataset (below 1.00%). (Priority 2: usage allowed, limited human intelligibility)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "bf_results"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'performance_discrimination': {'race': Amer-Indian-Eskimo    1.000000\n",
       "  Asian-Pac-Islander    0.510216\n",
       "  Other                 1.000000\n",
       "  Black                 0.639949\n",
       "  White                 0.562961\n",
       "  dtype: float64,\n",
       "  'sex': Male      0.591663\n",
       "  Female    0.526722\n",
       "  dtype: float64},\n",
       " 'proxy_identification': features\n",
       " relationship_sex    0.650656\n",
       " Name: association, dtype: float64,\n",
       " 'sensitive_predictability': race    0.121680\n",
       " sex     0.249346\n",
       " dtype: float64,\n",
       " 'sensitive_representativity': {'race': White                 0.8537\n",
       "  Black                 0.0978\n",
       "  Asian-Pac-Islander    0.0303\n",
       "  Other                 0.0092\n",
       "  Amer-Indian-Eskimo    0.0090\n",
       "  Name: race, dtype: float64,\n",
       "  'sex': Male      0.6657\n",
       "  Female    0.3343\n",
       "  Name: sex, dtype: float64}}"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the report, we know that we may have a proxy feature leaking information about a sensitive attribute (cf. PROXY IDENTIFICATION) and severe under-representation of feature values of a sensitive attribute. To investigate, we can fetch the warnings with the `get_warnings` method filtering for a specific test."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "# Looks like the 'relationship' and 'sex' features are highly correlated. Even if we removed the feature 'sex' from the data, \n",
    "# the 'relationship' feature could still leak information about the original sensitive attribute\n",
    "bf.get_warnings(test='Proxy Identification')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "QualityWarning(category='Bias&Fairness', test='Proxy Identification', description='Found 1 feature pairs of correlation to sensitive attributes with values higher than defined threshold (0.5).', priority=<Priority.P2: 2>, data=features\n",
       "relationship_sex    0.650656\n",
       "Name: association, dtype: float64)"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# From observing the data, we see that some relationship status (e.g. Husband, Wife) are gender-specific, thus impacting the correlation.\n",
    "df[['relationship', 'sex']].value_counts().sort_index()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "relationship    sex   \n",
       "Husband         Male      4023\n",
       "Not-in-family   Female    1221\n",
       "                Male      1351\n",
       "Other-relative  Female     132\n",
       "                Male       163\n",
       "Own-child       Female     712\n",
       "                Male       904\n",
       "Unmarried       Female     783\n",
       "                Male       215\n",
       "Wife            Female     495\n",
       "                Male         1\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Cleaning\n",
    "After the data quality issues have been detected, we can build a data processing pipeline with the guidance from the warnings raised above."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "def improve_quality(df: pd.DataFrame):\n",
    "    \"Clean the data based on the Data Quality issues found previously.\"\n",
    "    # Bias & Fairness\n",
    "    df = df.replace({'relationship': {'Husband': 'Married', 'Wife': 'Married'}}) # Substitute gender-based 'Husband'/'Wife' for generic 'Married'\n",
    "    \n",
    "    # Duplicates\n",
    "    df = df.drop(columns=['workclass2']) # Remove the duplicated column\n",
    "    df = df.drop_duplicates()            # Remove exact feature value duplicates\n",
    "\n",
    "    return df\n",
    "\n",
    "clean_df = improve_quality(df.copy())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Cleaned Data - DataQuality\n",
    "To check the impact of our data cleaning pipeline, we create a new DataQuality class now based on the improved version of the original data.\n",
    "Given that we removed the duplicated column and we erased the exact feature value duplicates, those warnings are not raised in the new DataQuality engine."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "%%capture\n",
    "better_dq = DataQuality(df=clean_df) # main class on cleaned data\n",
    "results = better_dq.evaluate() # run the tests"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "better_dq.report()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Warnings count by priority:\n",
      "\tPriority 2: 2 warning(s)\n",
      "\tTOTAL: 2 warning(s)\n",
      "List of warnings sorted by priority:\n",
      "\t[FLATLINES] Found 4165 flatline events with a minimun length of 5 among the columns {'relationship', 'workclass', 'marital-status', 'capital-gain', 'capital-loss', 'sex', 'education-num', 'income', 'education', 'native-country', 'occupation', 'hours-per-week', 'race'}. (Priority 2: usage allowed, limited human intelligibility)\n",
      "\t[PREDEFINED VALUED MISSING VALUES] Found 1360 vmvs in the dataset. (Priority 2: usage allowed, limited human intelligibility)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Cleaned Data - Specific Module\n",
    "For the specific analysis of Bias & Fairness, we see that the previous QualityWarning of \"Proxy Identification\" has disappeared. To check the new association results, we lower the threshold and observe that the association measure between 'relationship' and 'sex' features has dropped from 0.65 to 0.48."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "# Specific analysis for Bias & Fairness with improved dataframe\n",
    "better_bf = BiasFairness(df=better_df, sensitive_features=['race', 'sex'], label='income')\n",
    "_ = better_bf.evaluate()\n",
    "better_bf.report()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Warnings count by priority:\n",
      "\tPriority 2: 1 warning(s)\n",
      "\tTOTAL: 1 warning(s)\n",
      "List of warnings sorted by priority:\n",
      "\t[SENSITIVE ATTRIBUTE REPRESENTATIVITY] Found 2 values of 'race' sensitive attribute with low representativity in the dataset (below 1.00%). (Priority 2: usage allowed, limited human intelligibility)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "# The\n",
    "better_bf.proxy_identification(th=0.45)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "features\n",
       "relationship_sex      0.475097\n",
       "marital-status_sex    0.459768\n",
       "Name: association, dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The End\n",
    "That's it. In this quick tutorial, you learned how to use `ydata_quality` to assess the Data Quality of your dataset, both with the `DataQuality` main aggregator or through a specific module engine (e.g. `BiasFairness`). We introduced `QualityWarning`'s and how they provide a high-level measure of severity (cf. Priority) and contain the original data that raised the alarm. Based on the data quality insights, we defined a data cleaning pipeline and observed how it solved the warnings we aimed for."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('.venv': venv)"
  },
  "interpreter": {
   "hash": "cdc2bce73c2a9ac283f602628cabf735dbe06c4ee87a7849fc5f3d1177c8f304"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}