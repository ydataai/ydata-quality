{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# ydata-quality\n",
    "> The *ydata_quality* package aims to be for Data Quality what *sklearn* is for machine learning, *matplotlib* for visualization or *pandas* for data manipulation.\n",
    "\n",
    "Assess data quality throughout the multiple stages of a data pipeline development. Once you have a dataset available, running `DataQuality(df=my_df).report()` provides a comprehensive overview of the details and intricacies of the data, through the perspective of the multiple modules available in the package.\n",
    "\n",
    "For this tutorial, we will \n",
    "1. Load a dataset; \n",
    "2. Analyze its quality issues; \n",
    "3. Apply strategies to mitigate them;\n",
    "4. Check the new quality analysis on the post-processed (cleaned) data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Quick Start\n",
    "Load a DataFrame and evaluate your data using `DataQuality`.\n",
    "For more advanced analysis, we can provide additional arguments but we will get there in a minute."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "%%capture\n",
    "from ydata_quality import DataQuality\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(f'../datasets/transformed/census_10k.csv') # load data\n",
    "dq = DataQuality(df=df) # create the main class that holds all quality modules\n",
    "results = dq.evaluate() # run the tests"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "dq.report() # Output a report of the quality issues found by the engines"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Warnings count by priority:\n",
      "\tPriority 1: 1 warning(s)\n",
      "\tPriority 2: 4 warning(s)\n",
      "\tTOTAL: 5 warning(s)\n",
      "List of warnings sorted by priority:\n",
      "\t[DUPLICATE COLUMNS] Found 1 columns with exactly the same feature values as other columns. (Priority 1: heavy impact expected)\n",
      "\t[PREDEFINED ERRONEOUS DATA] Found 1960 ED values in the dataset. (Priority 2: usage allowed, limited human intelligibility)\n",
      "\t[HIGH COLLINEARITY - CATEGORICAL] Found 10 categorical variables with significant collinearity (p-value < 0.05). The variables listed in results are highly collinear with other variables in the dataset and sorted descending according to propensity. These will make model explainability harder and potentially give way to issues like overfitting. Depending on your end goal you might want to remove variables following the provided order. (Priority 2: usage allowed, limited human intelligibility)\n",
      "\t[EXACT DUPLICATES] Found 3 instances with exact duplicate feature values. (Priority 2: usage allowed, limited human intelligibility)\n",
      "\t[HIGH COLLINEARITY - NUMERICAL] Found 3 numerical variables with high Variance Inflation Factor (VIF>5.0). The variables listed in results are highly collinear with other variables in the dataset. These will make model explainability harder and potentially give way to issues like overfitting. Depending on your end goal you might want to remove the highest VIF variables. (Priority 2: usage allowed, limited human intelligibility)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/fsantos/miniconda3/envs/DQ/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the report, we get multiple warnings with different priorities (lower is of higher importance). The warnings were generated automatically by the default tests implemented in each module. From the report, we get an overall sense for each quality warning but this doesn't tell the whole story. To investigate the details, let's pick a QualityWarning and analyze it."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Warnings\n",
    "The warnings contain details for issues detected during the data quality analysis. For any given issue, the warning contains the information required by the Data Scientist to fully grasp the quality issue and how it is impacting the current dataset.\n",
    "\n",
    "Warnings can be fetched with `.get_warnings()` as a list of `QualityWarning`'s (best for coding) or summarized with `.report()` which print of overall status (best for analysis, visualization).\n",
    "\n",
    "Warnings are generated automatically for some tests but can be created by Data Scientists as well and added to existing engines."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "dq.get_warnings(test='Duplicate Columns')[0].data"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'workclass': ['workclass2']}"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the warning details, we know that the 'workclass2' feature is an exact copy of the original 'workclass'. For a typical machine learning pipeline, the duplicated feature 'workclass2' could be dropped as it is not adding any value (all info is already present) and may cause a toll in performance (e.g. due to collinearity effects)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Modules\n",
    "A full picture of data quality requires multiple perspectives, which we deliver in a modular way: Bias & Fairness, Data Expectations, Data Relations, Drift Analysis, Erroneous Data, Labelling and Missing. All of the engines are integrated into a single `DataQuality` class that allows you to run everything at once, providing a holistic perspective of your data. From the main `DataQuality` you can access the individual engines with the `.engines` property.\n",
    "\n",
    "Some of the modules will not run unless you provide specific arguments (e.g. target feature name for Labelling). By default, `DataQuality` will only contain the engines which have valid arguments and will drop all of those who are not sufficiently specified on the initialization.\n",
    "\n",
    "Since we didn't specify any sensitive features, the Bias&Fairness engine didn't run but we can define it now as a standalone as well."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from ydata_quality.bias_fairness import BiasFairness\n",
    "bf = BiasFairness(df=df, sensitive_features=['race', 'sex'], label='income')\n",
    "bf_results = bf.evaluate()\n",
    "bf.report()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Warnings count by priority:\n",
      "\tPriority 2: 2 warning(s)\n",
      "\tTOTAL: 2 warning(s)\n",
      "List of warnings sorted by priority:\n",
      "\t[PROXY IDENTIFICATION] Found 1 feature pairs of correlation to sensitive attributes with values higher than defined threshold (0.5). (Priority 2: usage allowed, limited human intelligibility)\n",
      "\t[SENSITIVE ATTRIBUTE REPRESENTATIVITY] Found 2 values of 'race' sensitive attribute with low representativity in the dataset (below 1.00%). (Priority 2: usage allowed, limited human intelligibility)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/fsantos/GitRepos/ydata-quality/src/ydata_quality/bias_fairness/engine.py:72: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  performances = pd.Series(index=self.sensitive_features)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "bf_results"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/fsantos/miniconda3/envs/DQ/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'performance_discrimination': \"[ERROR] Test failed to compute. Original exception: performance_per_feature_values() got an unexpected keyword argument 'target'\",\n",
       " 'proxy_identification': features\n",
       " relationship_sex    0.650656\n",
       " Name: association, dtype: float64,\n",
       " 'sensitive_predictability': \"[ERROR] Test failed to compute. Original exception: baseline_performance() got an unexpected keyword argument 'target'\",\n",
       " 'sensitive_representativity': {'race': White                 0.8537\n",
       "  Black                 0.0978\n",
       "  Asian-Pac-Islander    0.0303\n",
       "  Other                 0.0092\n",
       "  Amer-Indian-Eskimo    0.0090\n",
       "  Name: race, dtype: float64,\n",
       "  'sex': Male      0.6657\n",
       "  Female    0.3343\n",
       "  Name: sex, dtype: float64}}"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the report, we know that we may have a proxy feature leaking information about a sensitive attribute (cf. PROXY IDENTIFICATION) and severe under-representation of feature values of a sensitive attribute. To investigate, we can fetch the warnings with the `get_warnings` method filtering for a specific test."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Looks like the 'relationship' and 'sex' features are highly correlated. Even if we removed the feature 'sex' from the data, \n",
    "# the 'relationship' feature could still leak information about the original sensitive attribute\n",
    "bf.get_warnings(test='Proxy Identification')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[QualityWarning(category='Bias&Fairness', test='Proxy Identification', description='Found 1 feature pairs of correlation to sensitive attributes with values higher than defined threshold (0.5).', priority=<Priority.P2: 2>, data=features\n",
       " relationship_sex    0.650656\n",
       " Name: association, dtype: float64)]"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# From observing the data, we see that some relationship status (e.g. Husband, Wife) are gender-specific, thus impacting the correlation.\n",
    "df[['relationship', 'sex']].value_counts().sort_index()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "relationship    sex   \n",
       "Husband         Male      4023\n",
       "Not-in-family   Female    1221\n",
       "                Male      1351\n",
       "Other-relative  Female     132\n",
       "                Male       163\n",
       "Own-child       Female     712\n",
       "                Male       904\n",
       "Unmarried       Female     783\n",
       "                Male       215\n",
       "Wife            Female     495\n",
       "                Male         1\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Cleaning\n",
    "After the data quality issues have been detected, we can build a data processing pipeline with the guidance from the warnings raised above."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def improve_quality(df: pd.DataFrame):\n",
    "    \"Clean the data based on the Data Quality issues found previously.\"\n",
    "    # Bias & Fairness\n",
    "    df = df.replace({'relationship': {'Husband': 'Married', 'Wife': 'Married'}}) # Substitute gender-based 'Husband'/'Wife' for generic 'Married'\n",
    "    \n",
    "    # Duplicates\n",
    "    df = df.drop(columns=['workclass2']) # Remove the duplicated column\n",
    "    df = df.drop_duplicates()            # Remove exact feature value duplicates\n",
    "\n",
    "    return df\n",
    "\n",
    "clean_df = improve_quality(df.copy())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Cleaned Data - DataQuality\n",
    "To check the impact of our data cleaning pipeline, we create a new DataQuality class now based on the improved version of the original data.\n",
    "Given that we removed the duplicated column and we erased the exact feature value duplicates, those warnings are not raised in the new DataQuality engine."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "%%capture\n",
    "better_dq = DataQuality(df=clean_df) # main class on cleaned data\n",
    "results = better_dq.evaluate() # run the tests"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "better_dq.report()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Warnings count by priority:\n",
      "\tPriority 2: 3 warning(s)\n",
      "\tTOTAL: 3 warning(s)\n",
      "List of warnings sorted by priority:\n",
      "\t[HIGH COLLINEARITY - CATEGORICAL] Found 9 categorical variables with significant collinearity (p-value < 0.05). The variables listed in results are highly collinear with other variables in the dataset and sorted descending according to propensity. These will make model explainability harder and potentially give way to issues like overfitting. Depending on your end goal you might want to remove variables following the provided order. (Priority 2: usage allowed, limited human intelligibility)\n",
      "\t[PREDEFINED ERRONEOUS DATA] Found 1360 ED values in the dataset. (Priority 2: usage allowed, limited human intelligibility)\n",
      "\t[HIGH COLLINEARITY - NUMERICAL] Found 3 numerical variables with high Variance Inflation Factor (VIF>5.0). The variables listed in results are highly collinear with other variables in the dataset. These will make model explainability harder and potentially give way to issues like overfitting. Depending on your end goal you might want to remove the highest VIF variables. (Priority 2: usage allowed, limited human intelligibility)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/fsantos/miniconda3/envs/DQ/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Cleaned Data - Specific Module\n",
    "For the specific analysis of Bias & Fairness, we see that the previous QualityWarning of \"Proxy Identification\" has disappeared. To check the new association results, we lower the threshold and observe that the association measure between 'relationship' and 'sex' features has dropped from 0.65 to 0.48."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# Specific analysis for Bias & Fairness with improved dataframe\n",
    "better_bf = BiasFairness(df=clean_df, sensitive_features=['race', 'sex'], label='income')\n",
    "_ = better_bf.evaluate()\n",
    "better_bf.report()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Warnings count by priority:\n",
      "\tPriority 2: 1 warning(s)\n",
      "\tTOTAL: 1 warning(s)\n",
      "List of warnings sorted by priority:\n",
      "\t[SENSITIVE ATTRIBUTE REPRESENTATIVITY] Found 2 values of 'race' sensitive attribute with low representativity in the dataset (below 1.00%). (Priority 2: usage allowed, limited human intelligibility)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/fsantos/GitRepos/ydata-quality/src/ydata_quality/bias_fairness/engine.py:72: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  performances = pd.Series(index=self.sensitive_features)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# The\n",
    "better_bf.proxy_identification(th=0.45)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/fsantos/miniconda3/envs/DQ/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "features\n",
       "relationship_sex      0.475097\n",
       "marital-status_sex    0.459768\n",
       "Name: association, dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The End\n",
    "That's it. In this quick tutorial, you learned how to use `ydata_quality` to assess the Data Quality of your dataset, both with the `DataQuality` main aggregator or through a specific module engine (e.g. `BiasFairness`). We introduced `QualityWarning`'s and how they provide a high-level measure of severity (cf. Priority) and contain the original data that raised the alarm. Based on the data quality insights, we defined a data cleaning pipeline and observed how it solved the warnings we aimed for."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('DQ': conda)"
  },
  "interpreter": {
   "hash": "e255f3ac955330aecee05fff6b7b15b68f4bd4cf0e9481cf0822c8a2e5228d43"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}